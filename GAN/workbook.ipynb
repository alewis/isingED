{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmills/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Simple GAN implementation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "from IPython.display import clear_output\n",
    "import logging\n",
    "from ipywidgets.widgets.interaction import show_inline_matplotlib_plots\n",
    "%matplotlib inline\n",
    "\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStreamer(object):\n",
    "    \"\"\"\n",
    "    Class for reading csv data from the output file of generateconfigs.jl,\n",
    "    and providing shuffled batching.   \n",
    "    \"\"\"\n",
    "    def __init__(self, csvfile, fake=False):\n",
    "        with open(csvfile, 'rb') as F:\n",
    "            cleaned_lines = (line.replace(b';',b',') for line in F)\n",
    "            data  = np.genfromtxt(cleaned_lines, dtype=int, delimiter=',')\n",
    "            N = int(((data.shape[1] - 1) / 2))\n",
    "            configs = data[:,-(N+1):-1].reshape((-1,N))\n",
    "    \n",
    "        self.data = configs\n",
    "        self.data[self.data==0]=-1\n",
    "        self.example_shape = self.data.shape[1:]\n",
    "        self.epoch = -1\n",
    "        self.__new_epoch()\n",
    "        logging.info(\"Data shape: \" + str(self.data.shape))\n",
    "\n",
    "        \"\"\"Here I (conditionally) override the csv data with some 'easy'\n",
    "           data to test the GAN implementation\"\"\"\n",
    "        if False:\n",
    "            #Single pixel, always in the same place:  SUCCESS\n",
    "            self.data = np.zeros_like(self.data) - 1\n",
    "            self.data[:,1] = 1\n",
    "\n",
    "            #Vertical lines\n",
    "            self.data = np.zeros_like(self.data) - 1\n",
    "            for i in range(self.data.shape[0]):\n",
    "                d = self.data[i, :].reshape((4,4))\n",
    "                d[:,np.random.randint(0,4)] = 1\n",
    "                self.data[i,] = d.flatten()\n",
    "        \n",
    "        \n",
    "    def __new_epoch(self):\n",
    "        \"\"\"\n",
    "        Reset the queue of shuffled indices that will be removed as we\n",
    "        return examples.  Increment the epoch counter.\n",
    "        \"\"\"\n",
    "        self.epoch += 1\n",
    "        indices = np.arange(self.data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        self.q = list(indices)\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return batch_size examples in an array.\"\"\"\n",
    "        batch_data = np.zeros([batch_size,] + list(self.example_shape))\n",
    "        for i in range(batch_size):\n",
    "            index = self.q.pop()\n",
    "            batch_data[i,...] = self.data[index]\n",
    "            if len(self.q)==0:\n",
    "                self.__new_epoch()\n",
    "               \n",
    "        return batch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    \"\"\"\n",
    "       Construct the generator, taking a batch of random noise vectors and returning\n",
    "       a tensor representing a batch of generated examples.  Arbitrary tensorflow neural\n",
    "       network ops can be inserted here.\n",
    "    \"\"\"\n",
    "    def __init__(self, z, output_shape, scope='G'):\n",
    "        with tf.variable_scope(scope) as scope:\n",
    "            output = z\n",
    "            output = tf.layers.dense(output, 256, activation=tf.nn.leaky_relu)\n",
    "            output = tf.layers.dense(output, 512, activation=tf.nn.leaky_relu)\n",
    "            output = tf.layers.dense(output, 1024, activation=tf.nn.leaky_relu)\n",
    "            output = tf.layers.dense(output, 512, activation=tf.nn.leaky_relu)\n",
    "            output = tf.layers.dense(output, 256, activation=tf.nn.leaky_relu)\n",
    "            output = tf.layers.dense(output, 128, activation=tf.nn.leaky_relu)\n",
    "            output = tf.layers.dense(output, np.prod(output_shape), activation=tf.nn.tanh)            \n",
    "            self.output = tf.reshape(output, (-1,) + output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    \"\"\"\n",
    "       Construct the Discriminator, taking a batch of \"examples\" (either real or generated) \n",
    "       and returning an unactivated output indicating how \"real\" the example is considered.\n",
    "       Since this is unactivated, some refer to this as a \"Critic\" instead of \"Discriminator\".\n",
    "    \"\"\"\n",
    "    def __init__(self, x_in, scope='D'):\n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
    "            output = x_in\n",
    "            output = tf.layers.dense(output, 128, activation=tf.nn.leaky_relu)\n",
    "            output = tf.layers.dense(output, 256, activation=tf.nn.leaky_relu)\n",
    "            output = tf.layers.dense(output, 512, activation=tf.nn.leaky_relu)\n",
    "            output = tf.layers.dense(output, 1024, activation=tf.nn.leaky_relu)\n",
    "            output = tf.layers.dense(output, 1, activation=None)\n",
    "            self.output = output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random(object):\n",
    "    \"\"\"\n",
    "    Simple class for generating random latent noise.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def sample(self, shape):\n",
    "        return np.random.normal(loc=0.0, scale=0.1, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualiser(object):\n",
    "    \"\"\"\n",
    "    Simple plotting helper to plot some ground truth training examples beside the generator\n",
    "    output.  Use threshold=True to 'snap' the plot to -1 and 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        pass\n",
    "    def update(self, G_out, truth, threshold=False):\n",
    "        assert sample.shape[0]==self.batch_size,\"first dimension of sample must be equal to batch_size\"\n",
    "        N = int(np.sqrt(self.batch_size))\n",
    "        if threshold:\n",
    "            G_out[G_out < 0] = -1\n",
    "            G_out[G_out >= 0] = 1\n",
    "        N = min(N, 5)\n",
    "        fig, ax = plt.subplots(N,2*N,figsize=(20,10))\n",
    "        for k, d in enumerate([sample, ground_truth]):\n",
    "            counter = 0\n",
    "            for i in range(N):\n",
    "                for j in range(k*N,(k+1)*N):\n",
    "                    ax[i,j].pcolormesh(d[counter].reshape(4,4), cmap='gray', vmin=-1, vmax=1)\n",
    "                    ax[i,j].set_xticks([])\n",
    "                    ax[i,j].set_yticks([])\n",
    "                    counter += 1\n",
    "        fig.show()\n",
    "        show_inline_matplotlib_plots()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we instantiate and connect the pieces, define the loss functions, \n",
    "perform the weight clipping (for WGAN) on the discriminator, and set up the optimizer.\n",
    "\"\"\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "S = DataStreamer(\"../ising.csv\")\n",
    "\n",
    "z_ph = tf.placeholder(tf.float32, shape=(BATCH_SIZE, 128), name=\"input_noise\")\n",
    "x_ph = tf.placeholder(tf.float32, shape=tuple([BATCH_SIZE,] + list(S.example_shape)), name=\"true_sample\")\n",
    "lr = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n",
    "\n",
    "R = Random()\n",
    "G = Generator(z_ph, output_shape=S.example_shape)\n",
    "DR = Discriminator(x_ph)\n",
    "DG = Discriminator(G.output)\n",
    "V = Visualiser(BATCH_SIZE)\n",
    "\n",
    "D_loss = tf.reduce_mean(DR.output) - tf.reduce_mean(DG.output)\n",
    "G_loss = -tf.reduce_mean(DG.output)\n",
    "\n",
    "D_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='D')\n",
    "G_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='G')\n",
    "\n",
    "D_clip = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in D_vars]\n",
    "\n",
    "D_optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "G_optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "\n",
    "D_step = D_optimizer.minimize(-D_loss, var_list=D_vars)\n",
    "G_step = G_optimizer.minimize(G_loss, var_list=G_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 50 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   858\tGl: 1.020e-01\tDl: 2.332e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-52e093995dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         _, D_loss_val, _ = sess.run([D_step, D_loss, D_clip], \n\u001b[1;32m     22\u001b[0m                                 feed_dict = {x_ph: x_in,\n\u001b[0;32m---> 23\u001b[0;31m                                              \u001b[0mz_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_ph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                                              \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                             })\n",
      "\u001b[0;32m<ipython-input-5-349f7671e2da>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now the actual training.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Generate a random vector that will be used for plotting (so we can see the evolution\n",
    "of the generator on the same latent input.  Additionally we want the ground truth data\n",
    "to be the same in every frame.\n",
    "\"\"\"\n",
    "static_random = R.sample(z_ph.shape)\n",
    "ground_truth = S.next_batch(BATCH_SIZE)\n",
    "\n",
    "N = int(np.sqrt(BATCH_SIZE))\n",
    "\n",
    "\n",
    "for iteration in range(10000000):\n",
    "\n",
    "    for _ in range(5):\n",
    "        #train the Discriminator for five steps\n",
    "        x_in = S.next_batch(BATCH_SIZE)\n",
    "        _, D_loss_val, _ = sess.run([D_step, D_loss, D_clip], \n",
    "                                feed_dict = {x_ph: x_in,\n",
    "                                             z_ph: R.sample(z_ph.shape),\n",
    "                                             lr: 0.0001\n",
    "                                            })\n",
    "        \n",
    "    _, G_loss_val = sess.run([G_step, G_loss], feed_dict={z_ph:R.sample(z_ph.shape),\n",
    "                                                          lr: 0.0001})\n",
    "    if iteration % 10 == 0:\n",
    "        sample = sess.run(G.output, feed_dict={z_ph: static_random})\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        V.update(sample, ground_truth, threshold=False)\n",
    "        print(\"Epoch {:5d}\\tGl: {:5.3e}\\tDl: {:5.3e}\".format(S.epoch, G_loss_val, D_loss_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "After 10-ish minutes of training (~850 epochs), it arrives at something that doesn't look\n",
    "_completely_ wrong at first glance.  More work to be done though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
